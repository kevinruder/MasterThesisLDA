{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meget', 'med', 'mit', 'mine', 'den', 'under', 'jo', 'om', 'sig', 'ad', 'bliver', 'her', 'jeg', 'mig', 'noget', 'alle', 'var', 'man', 'selv', 'som', 'for', 'hun', 'også', 'sin', 'vi', 'fra', 'ud', 'thi', 'være', 'har', 'når', 'disse', 'og', 'da', 'han', 'skulle', 'de', 'vil', 'ikke', 'os', 'hvis', 'dog', 'et', 'din', 'til', 'op', 'eller', 'mange', 'efter', 'sådan', 'vor', 'i', 'dem', 'havde', 'af', 'blev', 'min', 'ham', 'på', 'sit', 'jer', 'hans', 'hvor', 'skal', 'dette', 'hvad', 'du', 'deres', 'det', 'end', 'have', 'hendes', 'anden', 'ned', 'ind', 'kunne', 'over', 'hos', 'er', 'en', 'blive', 'mod', 'men', 'ville', 'været', 'hende', 'denne', 'der', 'nogle', 'sine', 'at', 'nu', 'alt', 'dig'}\n",
      "[['teknologi', 'iot', 'nye', 'verden', 'virksomheder', 'allerede', 'internet', 'of', 'things', 'som...'], ['digitale', 'digitale', 'teknologi', 'heller'], ['siger', 'pct.', 'ligger', 'kan'], ['verden', 'godt', 'ved'], ['data', 'gennem', 'mener', 'data', 'første'], ['år', 'hvordan', 'første,', 'første,'], ['vestas', 'ordre', 'vestas', 'ordre', '76', 'danske', 'vestas'], ['ligger', 'digitale', 'pã¥', 'plads', 'lave', 'placering', 'pã¥,', 'komme', 'pã¥', 'er,', 'digitalisering', 'placering', 'ser', 'to', 'pã¥', 'lave', 'pã¥,', 'ser', 'digitalisering', 'kan', 'svarer', 'godt', 'resultaterne', 'analyse', 'di', 'monitor', 'deloitte', 'of'], ['særlige', 'ved', 'er,', 'data', 'steder.'], ['komme', 'sætte', 'mener'], ['danske', 'virksomheder', 'godt', 'med,', 'særlige', 'ved', 'ved', 'iot', '(internet', 'of'], ['lidt', 'resultaterne', 'første', 'danske', 'undersøgelse', 'danske', 'virksomheders', 'internetforbindelser', 'samarbejde', 'di', 'ericsson', 'fået', 'monitor'], ['internetforbindelser', 'godt', 'lidt', 'ny', 'undersøgelse', 'danske', 'virksomheders'], ['verden', 'seneste', 'år', 'fået', 'mere', 'mere', 'plads', 'danske', 'iot-tiltag', '(internet', 'of', 'selskaberne,', 'steder.', 'viser', 'ny', 'iot-tiltag,', 'udarbejdet', 'monitor', 'deloitte', 'ericsson', 'samarbejde', 'di', 'digital.', 'rapporten,', 'udarbejdet', 'gennem', 'danske', 'virksomheder,', 'blandt', '76', 'pct.', 'danske', 'virksomheder', 'iot-tiltag', 'komme', 'pct.', 'virksomheder', 'allerede', 'iot-tiltag,', 'blandt', 'andet', 'kan', 'produkter', 'analyse', 'pct.', 'iot-tiltag,', 'viser', 'stribe', 'danske', 'virksomheder,', 'rapporten,', 'føler', 'heller', 'godt', 'nye', 'svarer', 'selskaberne,', 'føler', 'rustet', 'iot-tiltag,', 'to', 'rustet', 'danske', 'virksomheder', 'fokus', 'iot', 'viser', 'virksomheder', 'komme', 'nye', 'til,', 'di', 'to', 'år', 'blandt', 'andet', 'sætte', 'fokus', 'siger', 'di', 'digital.'], ['internet', 'of', 'things', 'gennem', 'seneste', 'år', 'kan', 'virksomheders', 'produkter', 'som...'], ['søren', 'brogaard', 'jensen', 'komme', 'til,', 'hvordan', 'kan', 'internet', 'om,', 'søren', 'brogaard', 'jensen', 'fået'], ['danske', 'nye'], ['om,', 'stribe', 'med,']]\n",
      "stribe\n",
      "ordre\n",
      "verden\n",
      "nye\n",
      "data\n",
      "med,\n",
      "søren\n",
      "mener\n",
      "sætte\n",
      "ligger\n",
      "vestas\n",
      "brogaard\n",
      "siger\n",
      "pã¥\n",
      "pct.\n",
      "danske\n",
      "digitale\n",
      "om,\n",
      "ved\n",
      "ved\n",
      "jensen\n",
      "første,\n",
      "[(0, 0.29454758140583964), (1, 0.21057288804745836), (2, 0.3612020453496919), (3, 0.3612020453496919), (4, 0.29454758140583964), (5, 0.3612020453496919), (6, 0.29454758140583964), (7, 0.29454758140583964), (8, 0.3612020453496919), (9, 0.29454758140583964)]\n",
      "[(2, 0.408248290463863), (10, 0.816496580927726), (11, 0.408248290463863)]\n",
      "[(12, 0.547186837858363), (13, 0.547186837858363), (14, 0.547186837858363), (15, 0.3189979520681861)]\n",
      "[(4, 0.6310578980810031), (16, 0.4511450526595325), (17, 0.6310578980810031)]\n",
      "[(18, 0.7746945019849717), (19, 0.38734725099248585), (20, 0.38734725099248585), (21, 0.3158680783038782)]\n",
      "[(22, 0.34125967690157777), (23, 0.27828528037648265), (24, 0.8978302376545431)]\n",
      "[(25, 0.8098543290293163), (26, 0.5399028860195442), (27, 0.2052137216079723), (28, 0.10260686080398615)]\n",
      "[(1, 0.07403761885352655), (10, 0.12699896748667633), (12, 0.12699896748667633), (15, 0.07403761885352655), (16, 0.07403761885352655), (29, 0.3341253622335509), (30, 0.12699896748667633), (31, 0.12699896748667633), (32, 0.5011880433503263), (33, 0.3341253622335509), (34, 0.08693525385657722), (35, 0.3341253622335509), (36, 0.3341253622335509), (37, 0.10356319737343728), (38, 0.12699896748667633), (39, 0.12699896748667633), (40, 0.3341253622335509), (41, 0.12699896748667633), (42, 0.12699896748667633), (43, 0.10356319737343728), (44, 0.12699896748667633)]\n",
      "[(17, 0.37755500256837193), (18, 0.46299357987870543), (31, 0.46299357987870543), (45, 0.46299357987870543), (46, 0.46299357987870543)]\n",
      "[(20, 0.6364666670617053), (34, 0.435683788358861), (47, 0.6364666670617053)]\n",
      "[(0, 0.2897696910372511), (1, 0.20715716088756836), (7, 0.2897696910372511), (16, 0.20715716088756836), (17, 0.5795393820745022), (28, 0.17767147260800473), (45, 0.35534294521600945), (48, 0.35534294521600945), (49, 0.35534294521600945)]\n",
      "[(19, 0.30628292480044267), (28, 0.30628292480044267), (37, 0.24976296753396557), (41, 0.30628292480044267), (43, 0.24976296753396557), (50, 0.24976296753396557), (51, 0.30628292480044267), (52, 0.30628292480044267), (53, 0.30628292480044267), (54, 0.30628292480044267), (55, 0.30628292480044267), (56, 0.24976296753396557)]\n",
      "[(16, 0.25431515293303175), (28, 0.21811723782343667), (51, 0.43623447564687334), (52, 0.43623447564687334), (55, 0.43623447564687334), (56, 0.3557338929330645), (57, 0.43623447564687334)]\n",
      "[(0, 0.2304247626534209), (1, 0.04118282648765537), (3, 0.07064214818229604), (4, 0.057606190663355226), (6, 0.11521238132671045), (7, 0.057606190663355226), (11, 0.07064214818229604), (13, 0.07064214818229604), (14, 0.21192644454688814), (15, 0.04118282648765537), (16, 0.04118282648765537), (21, 0.057606190663355226), (23, 0.11521238132671045), (27, 0.07064214818229604), (28, 0.1766053704557401), (30, 0.07064214818229604), (34, 0.09671406322017766), (37, 0.057606190663355226), (38, 0.07064214818229604), (39, 0.07064214818229604), (42, 0.07064214818229604), (43, 0.17281857199006567), (44, 0.14128429636459208), (46, 0.07064214818229604), (47, 0.07064214818229604), (49, 0.07064214818229604), (50, 0.057606190663355226), (53, 0.07064214818229604), (54, 0.07064214818229604), (57, 0.07064214818229604), (58, 0.18585452950900647), (59, 0.27878179426350974), (60, 0.18585452950900647), (61, 0.07064214818229604), (62, 0.18585452950900647), (63, 0.18585452950900647), (64, 0.27878179426350974), (65, 0.18585452950900647), (66, 0.18585452950900647), (67, 0.07064214818229604), (68, 0.18585452950900647), (69, 0.18585452950900647), (70, 0.37170905901801293), (71, 0.07064214818229604), (72, 0.18585452950900647), (73, 0.18585452950900647), (74, 0.07064214818229604), (75, 0.18585452950900647)]\n",
      "[(1, 0.21518615037445693), (5, 0.36911531378466256), (8, 0.36911531378466256), (9, 0.3010005738751354), (15, 0.21518615037445693), (21, 0.3010005738751354), (23, 0.3010005738751354), (56, 0.3010005738751354), (61, 0.36911531378466256), (71, 0.36911531378466256)]\n",
      "[(9, 0.16022253149352736), (15, 0.1145435349556668), (22, 0.19647999080606313), (34, 0.13449745471556732), (50, 0.16022253149352736), (74, 0.19647999080606313), (76, 0.5169250537931178), (77, 0.19647999080606313), (78, 0.5169250537931178), (79, 0.5169250537931178)]\n",
      "[(6, 0.852508574458651), (28, 0.5227132392378048)]\n",
      "[(48, 0.5773502691896258), (67, 0.5773502691896258), (77, 0.5773502691896258)]\n",
      "[('data', 0.055029091667827469), ('er,', 0.05502885018501278), ('særlige', 0.055028177956617526), ('steder.', 0.055027142609328998), ('ved', 0.046681666629590744), ('danske', 0.0097783621072635423), ('digitale', 0.0097776040548684749), ('mener', 0.0097774978523477146), ('første,', 0.009777411640893359), ('nye', 0.0097773420837144326)]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import logging\n",
    "from gensim import corpora,models, similarities\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "\n",
    "stop_words = get_stop_words('danish')\n",
    "\n",
    "\n",
    "\n",
    "def get_URL(list_of_a):\n",
    "    list_of_url = []\n",
    "\n",
    "    for link in list_of_a:\n",
    "        list_of_url.append(link.a.get(\"href\"))\n",
    "\n",
    "    return list_of_url\n",
    "\n",
    "def extract_body_of_text(url_list):\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for link in url_list:\n",
    "        r = requests.get(link)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        body_text = soup.find_all(\"div\", itemprop=\"articleBody\")\n",
    "\n",
    "        for item in body_text:\n",
    "            documents.append(item.text)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def extract_tfidf_index(corpus_tfidf):\n",
    "\n",
    "    #extracts all indexes/words with a TFIDF of above 0.5\n",
    "\n",
    "    indexes = []\n",
    "\n",
    "    for doc in corpus_tfidf:\n",
    "        for a in doc:\n",
    "            #0.5 is the TFIDF THRESHOLD AND IS HARDCODED ATM, SHOULD BE A VARIABLE THAT IS DECLARED ON TOP of code\n",
    "            if (a[1] > 0.5):\n",
    "                indexes.append(a[0])\n",
    "\n",
    "    return indexes\n",
    "\n",
    "def find_word_with_index(list_of_index,word_identifier_list):\n",
    "\n",
    "    #looks at the list of indexes and prints out the word that is connected to the Identifier\n",
    "\n",
    "    for doc in word_identifier_list:\n",
    "        for index in list_of_index:\n",
    "            if word_identifier_list[doc] == index:\n",
    "                print(doc)\n",
    "\n",
    "#declare URL\n",
    "#declare page NR\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "r = requests.get(\"https://borsen.dk/soegning.html?query=IOT\")\n",
    "\n",
    "#THEN ONTO PAGE 2 https://borsen.dk/soegning.html?query=IOT&page=2\n",
    "\n",
    "soup = BeautifulSoup(r.content,'html.parser')\n",
    "\n",
    "total_score = 0\n",
    "\n",
    "list_of_links = soup.find_all(\"li\", {\"class\": \"padding-all\"})\n",
    "\n",
    "list_of_url  = get_URL(list_of_links)\n",
    "\n",
    "documents = extract_body_of_text(list_of_url)\n",
    "\n",
    "#stoplist = set('den er på af fra og til ind det i hvis hvad du kan har ikke om men  med en hos for de der deres at'.split()) # change stopwords to danish\n",
    "\n",
    "stoplist = set(stop_words)\n",
    "\n",
    "print(stoplist)\n",
    "\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]for document in documents]\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]for text in texts]\n",
    "\n",
    "print(texts)\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "dictionary.save('temporary.dict')\n",
    "\n",
    "word_identifier_list = dictionary.token2id\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "corpora.MmCorpus.serialize('temporary.mm',corpus)\n",
    "\n",
    "#for poop in corpus:\n",
    "   #print(poop)\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "list_of_index = extract_tfidf_index(corpus_tfidf)\n",
    "\n",
    "find_word_with_index(list_of_index,word_identifier_list)\n",
    "\n",
    "#Have to swap between keys and values in dict or else LDAmodel Will not recognize the format\n",
    "new_dict = {y:x for x,y in word_identifier_list.items()}\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)\n",
    "\n",
    "\n",
    "lsi = models.LdaModel(corpus = corpus_tfidf, num_topics=10 ,id2word=new_dict)\n",
    "\n",
    "\n",
    "\n",
    "lsi.print_topics(10)\n",
    "\n",
    "print(lsi.show_topic(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-365c9bfeb7d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pip' is not defined"
     ]
    }
   ],
   "source": [
    "pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
